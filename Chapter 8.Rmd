---
title: "Chapter 8"
author: "Tong Sun"
date: "2/27/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="pdf",fig.align  = 'center')
```

### 8.1
Draw an example (of your own invention) of a partition of two-dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all aspects of your figures, including the regions $R_1$, $R_2$,..., the cutpoints $t_1$, $t_2$,..., and so forth.
Hint: Your result should look something like Figures 8.1 and 8.2.

![my answer](/Users/christinasun/Desktop/MA679/Assignment/8.1.jpeg)
### 8.2
It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form $f(X)=\sum_{j=1}^{p} f_j(X_j)$.
Explain why this is the case. You can begin with (8.12) in Algorithm 8.2.

![my answer](/Users/christinasun/Desktop/MA679/Assignment/8.2.jpeg)

### 8.3
Consider the Gini index, classification error, and entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a function of $\hat p_{m1}$ The x axis should display $\hat p_{m1}$, ranging from 0 to 1, and the y axis should display the value of the Gini index, classification error, and entropy.
Hint: In a setting with two classes, $\hat p_{m1} = 1-\hat p_{m2}$. You could make this plot by hand, but it will be much easier to make in R.

```{r echo=FALSE}
p <- seq(0, 1, 0.01)
gini.index <- 2*p*(1-p)
class.error <- 1 - pmax(p, 1-p)
cross.entropy <- -(p*log(p) + (1-p)*log(1-p))
matplot(p, cbind(gini.index, class.error, cross.entropy), col = c("red", "green","blue"))
```

### 8.5
Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of X, produce 10 estimates of P(Class is Red): 0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75. There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches?

With the majority vote approach, the conclusion of this question is X is red because there are 6 for red and 4 for green among these 10 predictions. And with the average probability approach, the conclusion is that X is green, because the average of these 10 probabilities is $\frac{0.1+0.15+0.2+0.2+0.55+0.6+0.6+0.65+0.7+0.75}{10}=0.45$, which is lower than 0.5.

### 8.7
In the lab, we applied random forests to the Boston data using "mtry = 6" and using "ntree = 25" and "ntree = 500". Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for "mtry" and "ntree". You can model your plot after Figure 8.10. Describe the results obtained.

```{r echo=FALSE}
library(randomForest)
library(MASS)
set.seed(1000)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
x.train <- Boston[train, -14]
x.test <- Boston[-train, -14]
y.train <- Boston[train, 14]
y.test <- Boston[-train, 14]

forest.p1 <- randomForest(x.train, y.train, xtest = x.test, ytest = y.test, mtry = ncol(Boston) -1 , ntree = 500)
forest.p2 <- randomForest(x.train, y.train, xtest = x.test, ytest = y.test, mtry = (ncol(Boston) -1) / 2, ntree = 500)
forest.p.sqrt <- randomForest(x.train, y.train, xtest = x.test, ytest = y.test, mtry = sqrt(ncol(Boston) -1), ntree = 500)

plot(1:500, forest.p1$test$mse, col = "blue", type = "l", xlab = "Number of Trees", ylab = "Test MSE", ylim = c(8, 19))
lines(1:500, forest.p2$test$mse, col = "red", type = "l")
lines(1:500, forest.p.sqrt$test$mse, col = "green", type = "l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col = c("blue", "red", "green"), cex = 1, lty = 1)
```
Results from random forests for the data set with p = 500 predictors. The test error is displayed as a function of the number of trees. Each colored line corresponds to a different value of m, the number of predictors available for splitting at each interior tree node. Random forests ($m=p/2$) lead to a slight improvement over bagging (m=p). We may also see that the Test MSE is very high for a single tree, it decreases as the number of trees increases. But from this plot, the difference between these three Test MSEs are not so significant although the Test MSE for all predictors is higher than for half the predictors or the square root of the number of predictors.

### 8.8
In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.
##(a)
Split the data set into a training set and a test set.
```{r echo=FALSE}
library(ISLR)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
```

##(b)
Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?
```{r echo=FALSE}
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
```
Here I fit a regression tree to the Carseats data set. First, I create a training set and fit the tree to the training data. The output of "summary()" indicates that only seven variables have been used in constructing the tree. The deviance is simply the sum of squared errors for the tree.

```{r echo=FALSE}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```
From the regression tree plot, the split at the top of the tree results in two large branches. The left-hand branch corresponds to "ShelveLoc" in "Good" level, and the right-hand branch corresponds to "ShelveLoc" in "Bad and Medium" level. And also the predictor "ShelveLoc" is the most important factor in determining "Sales". The number in each leaf is the mean of the response for the observations that fall there.

```{r echo=FALSE}
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
```
The conclusion is that the Test MSE is about 4.35.

##(c)
Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
```{r echo=FALSE}
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "blue", cex = 2, pch = 20)
```
The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex. So here I use the "cv.tree()" function to see whether pruning the tree will improve performance.In this case, the most complex tree under consideration is selected by cross-validation.  In this case, the tree of size 8 is selected by cross-validation. We now prune the tree to obtain the 8-node tree.
```{r echo=FALSE}
prune.carseats <- prune.tree(tree.carseats, best = 8)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```
```{r echo=FALSE}
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
```
I find that pruning the tree increases the Test MSE to 4.68.

##(d)
Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.
```{r echo=FALSE}
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
```
I find that bagging decreases the Test MSE to 2.48.
```{r echo=FALSE}
importance(bag.carseats)
```
We may conclude that "ShelveLoc" and "Price" are the two most important variables.

##(e)
Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.
```{r echo=FALSE}
rf.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 3, ntree = 500, importance = TRUE)
yhat.rf <- predict(rf.carseats, newdata = Carseats.test)
mean((yhat.rf - Carseats.test$Sales)^2)
```
Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, here I build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors.The split is allowed to use only one of those m predictors.In this case, with m = $\sqrt{p}$, we have a Test MSE of 2.94, which leads to a reduction in test error.
```{r echo=FALSE}
importance(rf.carseats)
```
In this case, the two most important variables are also the "Price" and "ShelveLoc".

##(f)
Now analyze the data using BART, and report your results.
```{r echo=FALSE}
library(BART)
x <- Carseats[ , 2:11]
y <- Carseats[ , "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
```
```{r echo=FALSE}
# Compute the test error
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2)
```
On this data set, the test error of BART is 1.62, lower than the test error of random forests and boosting. Next I check how many times each variable appeared in the collection of trees.
```{r echo=FALSE}
ord <- order(bartfit$varcount.mean, decreasing = T)
bartfit$varcount.mean[ord]
```

### 8.11
This question uses the Caravan data set.
##(a)
Create a training set consisting of the first 1,000 observations,and a test set consisting of the remaining observations.
```{r echo=FALSE}
set.seed(1)
train <- 1:1000
Caravan$Purchase <- ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train <- Caravan[train, ]
Caravan.test <- Caravan[-train, ]
```

##(b)
Fit a boosting model to the training set with "Purchase" as the response and the other variables as predictors. Use 1,000 trees,and a shrinkage value of 0.01. Which predictors appear to be the most important?
```{r echo=FALSE}
library(gbm)
set.seed(1)
boost.caravan <- gbm(Purchase ~ ., data = Caravan.train, distribution = "gaussian", n.trees = 1000, shrinkage = 0.01)
summary(boost.caravan)
```
Here I use the "gbm" package, and within it the "gbm()" function, to fit boosted regression trees to the Caravan data set. I run "gbm()" with the option "distribution = 'gaussian'" since this is a regression problem. The argument "n.tree = 1000" indicates that here we want 1000 trees. The "summary()" function produces a relative plot and also outputs the relative influence statistics.The variables "PPERSAUT" and "MKOOPKLA" are the two most important variables.
```{r echo=FALSE}
plot(boost.caravan, i = "PPERSAUT")
plot(boost.caravan, i = "MKOOPKLA")
```
Here I also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables.

##(c)
Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20%. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?
```{r echo=FALSE}
probs.test <- predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response")
pred.test <- ifelse(probs.test > 0.2, 1, 0)
table(Caravan.test$Purchase, pred.test)
```

For boosting, the fraction of people predicted to make a purchase that in fact make one is 0.2156863.

```{r echo=FALSE}
logit.caravan <- glm(Purchase ~ ., data = Caravan.train, family = "binomial")
probs.test2 <- predict(logit.caravan, Caravan.test, type = "response")
pred.test2 <- ifelse(probs.test > 0.2, 1, 0)
table(Caravan.test$Purchase, pred.test2)
```

For logistic regression, the fraction of people predicted to make a purchase that in fact make one is again 0.2156863.