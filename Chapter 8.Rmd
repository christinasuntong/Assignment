---
title: "Chapter 8"
author: "Tong Sun"
date: "2/27/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="pdf",fig.align  = 'center')
```

### 8.1
Draw an example (of your own invention) of a partition of two-dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a
decision tree corresponding to this partition. Be sure to label all aspects of your figures, including the regions $R_1$, $R_2$,..., the cutpoints $t_1$, $t_2$,..., and so forth.
Hint: Your result should look something like Figures 8.1 and 8.2.

![my answer](/Users/christinasun/Desktop/MA679/Assignment/8.1.jpeg)
### 8.2
It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form $f(X)=\sum_{j=1}^{p} f_j(X_j)$.
Explain why this is the case. You can begin with (8.12) in Algorithm 8.2.


### 8.3
Consider the Gini index, classification error, and entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a function of $\hat p_{m1}$ The x-axis should display $\hat p_{m1}$, ranging from 0 to 1, and the y-axis should display the value of the Gini index, classification error, and entropy.
Hint: In a setting with two classes, $\hat p_{m1}$ = 1 âˆ’ $\hat p_{m2}$. You could make this plot by hand, but it will be much easier to make in R.
```{r}
p <- seq(0, 1, 0.01)
gini.index <- 2*p*(1-p)
class.error <- 1 - pmax(p, 1-p)
cross.entropy <- -(p*log(p) + (1-p)*log(1-p))
matplot(p, cbind(gini.index, class.error, cross.entropy), col = c("red", "green","blue"))
```

### 8.5
Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of X, produce 10 estimates of P(Class is Red|X): 0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75. There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches?

With the majority vote approach, the conclusion of this question is X is red because there are 6 for red and 4 for green among these 10 predictions. And with the average probability approach, the conclusion is that X is green, because the average of these 10 probabilities is $\frac{0.1+0.15+0.2+0.2+0.55+0.6+0.6+0.65+0.7+0.75}{10}=0.45$, which is lower than 0.5.

### 8.7
In the lab, we applied random forests to the Boston data using "mtry = 6" and using "ntree = 25" and "ntree = 500". Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for "mtry" and "ntree". You can model your plot after Figure 8.10. Describe the results obtained.
```{r}
library(randomForest)
library(MASS)
set.seed(2022)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
x.train <- Boston[train, -14]
x.test <- Boston[-train, -14]
y.train <- Boston[train, 14]
y.test <- Boston[-train, 14]

forest.p1 <- randomForest(x.train, y.train, xtest = x.test, ytest = y.test, mtry = ncol(Boston) -1 , ntree = 500)
forest.p2 <- randomForest(x.train, y.train, xtest = x.test, ytest = y.test, mtry = (ncol(Boston) -1) / 2, ntree = 500)
forest.p.sqrt <- randomForest(x.train, y.train, xtest = x.test, ytest = y.test, mtry = sqrt(ncol(Boston) -1), ntree = 500)

plot(1:500, forest.p1$test$mse, col = "blue", type = "l", xlab = "Number of Trees", ylab = "Test MSE", ylim = c(10, 19))
lines(1:500, forest.p2$test$mse, col = "red", type = "l")
lines(1:500, forest.p.sqrt$test$mse, col = "green", type = "l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col = c("blue", "red", "green"), cex = 1, lty = 1)
```
We may see that the Test MSE is very high for a single tree, it decreases as the number of trees increases. Also the Test MSE for all predictors is higher than for half the predictors or the square root of the number

### 8.8
### 8.11