---
title: "Chapter 7"
author: "Tong Sun"
date: "2/13/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="pdf",fig.align  = 'center')
```

### 7.3
Suppose we fit a curve with basis functions $b_1(X)=X$, $b_2(X)=(X-1)^2I(X>=1)$. (Note that $I(X>=1)$ equals 1 for X>=1 and 0 otherwise.) We fit the linear regression model$Y=\beta_0+\beta_1b_1(X)+\beta_2b_2(X)+\epsilon$, and obtain coefficient estimates $\hat\beta_0=1$,$\hat\beta_1=1$,$\hat\beta_2=-2$. Sketch the estimated curve between X=-2 and X=2. Note the intercepts, slopes and other relevant information.
```{r echo=FALSE}
x <- -2:2
y <- 1 + x + -2*(x-1)^2 * I(x>1)
plot(x,y)
```
The curve is linear between-2 and 1: $y=1+x$ and quadratic between 1 and 2:$y=1+x-2(x-1)^2$.

### 7.9
This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response.
##(a)
Use the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.
```{r echo=FALSE}
library(MASS)
set.seed(1)
fit <- lm(nox ~ poly(dis, 3), data = Boston)
summary(fit)
```
Here the poly() command allows us to avoid having to write out a long formula with powers of dis. The function returns a matrix whose columns are a basis of orthogonal polynomials, which essentially means that each column is a linear combination of the variables $dis$, $dis^2$ and $dis^3$.

```{r echo=FALSE}
dislims <- range(Boston$dis)
dis.grid <- seq(from = dislims[1], to = dislims[2], by = 0.1)
preds <- predict(fit, newdata = list(dis = dis.grid), se = TRUE)
se.bands <- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
par(mfrow = c(1,2), mar = c(4.5,4.5,1,1), oma = c(0, 0, 4, 0))
plot(Boston$dis, Boston$nox, xlim = dislims, cex = .5, col = "darkgrey")
title("Degree-3 Polynomial", outer = T)
lines(dis.grid, preds$fit, col = "red", lwd = 2)
matlines(dis.grid, se.bands, lwd = 1, col = "red", lty = 3)
```

Here I create a grid of values for dis at which I want predictions. And then I plot the data and add the fit from the degree-3 polynomial. Here the $mar$ and $oma$ arguments to $par()$ allow me to control the margins of the plot, and the $title()$ function creates a figure title that spans both subplots. We may conclude that all polynomial terms are significant.

##(b)
Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.
```{r echo=FALSE}
rss <- rep(NA, 10)
for(i in 1:10){
  fit <- lm(nox ~ poly(dis, i), data = Boston)
  rss[i] <- sum(fit$residuals^2)
}
plot(1:10, rss, xlab = "Degree", ylab = "RSS", type = "l")
```
It seems that the RSS decreases with the degree of the polynomial, and so is minimum for a polynomial of degree 10.

##(c)
Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.
```{r echo=FALSE}
library(boot)
deltas <- rep(NA, 10)
for (i in 1:10){
  fit <- glm(nox ~ poly(dis, i), data = Boston)
  deltas[i] <- cv.glm(Boston, fit, K = 10)$delta[1]
}
plot(1:10, deltas, xlab = "Degree", ylab = "Test MSE", type = "l")
```

We may see that a polynomial of degree 4 minimizes the test MSE.

##(d)
Use the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.
```{r echo=FALSE}
library(splines)
fit <- lm(nox ~ bs(dis, knots = c(4, 7, 11)), data = Boston)
summary(fit)
```
```{r echo=FALSE}
pred <-predict(fit, list(dis = dis.grid))
plot(nox ~ dis, data = Boston, col = "darkgrey")
lines(dis.grid, preds$fit, col = "red", lwd = 2)
```

We may conclude that all terms in spline fit are significant.

##(e)
Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.
```{r echo=FALSE}
rss <- rep(NA, 16)
for(i in 3:16){
  fit <- lm(nox ~ bs(dis, df = i), data = Boston)
  rss[i] <- sum(fit$residuals^2)
}
plot(3:16, rss[-c(1,2)], xlab = "Degrees of freedom", ylab = "RSS", type = "l")
```
We may see that RSS decreases until 14 and then slightly increases after that.

##(f)
Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data.Describe your results.
```{r echo=FALSE}
library(boot)
cv <- rep(NA, 16)
for(i in 3:16){
  fit <- glm(nox ~ bs(dis, df = i), data = Boston)
  cv[i] <- cv.glm(Boston, fit, K = 10)$delta[1]
}
```
```{r echo=FALSE}
plot(3:16, cv[-c(1, 2)], xlab = "Degrees of freedom", ylab = "Test MSE", type = "l")
```
Test MSE is minimum for 10 degrees of freedom.

### 7.10
This question relates to the College data set.
##(a)
Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.
```{r echo=FALSE}
library(leaps)
library(ISLR)
set.seed(1)
attach(College)
train <- sample(length(Outstate), length(Outstate) / 2)
test <- -train
College.train <- College[train, ]
College.test <- College[test, ]
fit.fwd <- regsubsets(Outstate ~ ., data = College.train, nvmax = 17, method = "forward")
fit.summary <- summary(fit.fwd)
par(mfrow = c(1,3))
plot(fit.summary$cp, xlab = "Number of variables", ylab = "Cp", type = "l")
plot(fit.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
plot(fit.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R2", type = "l", ylim = c(0.4, 0.84))
```
From the plots above, I think size six is the minimum size for the subset.

```{r echo=FALSE}
fit <- regsubsets(Outstate ~ ., data = College, method = "forward")
coeffs <- coef(fit, id = 6)
names(coeffs)
```

##(b)
Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.
```{r echo=FALSE}
library(gam)
fit.gam <- gam(Outstate ~ Private + s(Room.Board,2) + s(PhD,2) + s(perc.alumni,2) + s(Expend,5) + s(Grad.Rate,2), data = College.train)
par(mfrow = c(2,3))
plot(fit.gam, se = TRUE, col = "blue")
``` 
I specify that the function of "Room.Board" should have two degrees of freedom, the function of "PhD" should have two degrees of freedom, the function of "perc.alumni" should have two degrees of freedom, the function of "Expend" should have five degrees of freedom and the function of "Grad.Rat" should have five degrees of freedom. Since "Private" is qualitative, I leave it as is and it is converted into dummy variables.For the College data, plots of the relationship between each feature and the response, out of state tuition, in the fitted model. Each plot displays the fitted function and pointwise standard errors.Each panel indicates that holding all the other predictors (without the x-axis one) fixed, the relationship between out-of-state tuition and predictor.For the "Room.Board", "PhD", "perc.alumni", "Expend" and "Grad.Rat" predictors, with holding other five variables fixed, tuition tends to increase with each predictor. And for "Private" variable, holding other variables fixed, those with answer "Yes" tends to pay more tuition than those saying "No".

##(c)
Evaluate the model obtained on the test set, and explain the results obtained.
```{r echo=FALSE}
preds <- predict(fit.gam, College.test)
err <- mean((College.test$Outstate - preds)^2)
err
```

```{r echo=FALSE}
tss <- mean((College.test$Outstate - mean(College.test$Outstate))^2)
rss <- 1-err/tss
rss
```

We obtain a test R^2 of 0.77 using GAM with six predictors.

##(d)
For which variables, if any, is there evidence of a non-linear relationship with the response?
```{r echo=FALSE}
summary(fit.gam)
```
The "Anova for Parametric Effects" p-values clearly demonstrate that "Private", "Room.Board","PhD", "perc.alumni","Expend" and "Grad.Rate" are all highly statistically significant, even when only assuming a linear relationship. Alternatively, the "Anova for Nonparametric Effects" p-values for predictors correspond to a null hypothesis of a linear relationship versus the alternative of a non-linear relationship. The large p-value for "Room.Board", "PhD", "perc.alumni" and "Grad.Rate" reinforces the conclusion from the Anova test that a linear function is adequate for these terms. However, there is very clear evidence that a non-linear term is required for "Expend".

### 7.11
In Section 7.7, it was mentioned that GAMs are generally fit using a backfitting approach. The idea behind backfitting is actually quite simple. We will now explore backfitting in the context of multiple linear regression.
Suppose that we would like to perform multiple linear regression, but we do not have software to do so. Instead, we only have software to perform simple linear regression. Therefore, we take the following iterative approach: we repeatedly hold all but one coefficient estimate fixed at its current value, and update only that coefficient estimate using a simple linear regression. The process is continued until convergence—that is, until the coefficient estimates stop changing.
We now try this out on a toy example.
##(a)
Generate a response Y and two predictors $X_1$ and $X_2$, with n = 100.
```{r echo=FALSE}
set.seed(1)
y <- rnorm(100)
x1 <-rnorm(100)
x2 <-rnorm(100)
```


##(b)
Initialize $\hat \beta_1$ to take on a value of your choice. It does not matter
what value you choose.
```{r echo=FALSE}
beta1 <- 2.15
```

##(c)
Keeping $\hat \beta_1$ fixed, fit the model: $Y-\hat \beta_1X_1=\beta_0+\beta_2X_2+\epsilon$
```{r echo=FALSE}
a <- y - beta1 * x1
beta2 <- lm(a ~ x2)$coef[2]
```

##(d)
Keeping $\hat \beta_2$ fixed, fit the model: $Y-\hat \beta_2X_2=\beta_0+\beta_1X_1+\epsilon$
```{r echo=FALSE}
a <- y - beta2 * x2
beta1 <- lm(a ~ x1)$coef[2]
```

##(e)
Write a for loop to repeat (c) and (d) 1,000 times. Report the estimates of $\hat\beta_0$, $\hat\beta_1$, and $\hat\beta_2$ at each iteration of the for loop. Create a plot in which each of these values is displayed, with $\hat\beta_0$, $\hat\beta_1$, and $\hat\beta_2$ each shown in a different color.
```{r echo=FALSE,include=FALSE}
times <- 1000
df <-data.frame(0, 2.55, 0)
names(df) <- c('beta0','beta1','beta2')
for (i in 1:times) {
  beta1 <-df[nrow(df), 2]
  a <- y - beta1 * x1
  beta2 <-lm(a ~ x2)$coef[2]
  a <- y - beta2 * x2
  beta1 <- lm(a ~ x1)$coef[2]
  beta0 <- lm(a ~ x1)$coef[1]
  print(beta0)
  print(beta1)
  print(beta2)
  df[nrow(df) + 1,] <- list(beta0,beta1,beta2)
}
```

```{r echo=FALSE}
plot(df$beta0, col = 'red', type = 'l')
lines(df$beta1, col = 'blue')
lines(df$beta2, col = 'green')
```

##(f)
Compare your answer in (e) to the results of simply performing multiple linear regression to predict Y using $X_1$ and $X_2$. Use the abline() function to overlay those multiple linear regression coefficient estimates on the plot obtained in (e).
```{r echo=FALSE}
plot(df$beta0, col = 'red', type = 'l')
lines(df$beta1, col = 'blue')
lines(df$beta2, col = 'green')
multiple<- coef(lm(y ~ x1 + x2))
abline(h = multiple[1], col = 'darkred', lty = 2)
abline(h = multiple[2], col = 'darkblue', lty = 2)
abline(h = multiple[3], col = 'darkgreen', lty = 2)
```
The coefficients from multiple linear regression and iterations are almost the same.

##(g)
On this data set, how many backfitting iterations were required in order to obtain a “good” approximation to the multiple regression coefficient estimates?

From the for loop in 1000 times,it is difficult to see the exactly backfitting iterations. So I run a for loop in 10 times again, I find that for the 10-times loop, three iterations are enough to converge.