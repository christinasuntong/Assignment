---
title: "Chapter3"
author: "Tong Sun"
date: "1/23/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="pdf",fig.align  = 'center')
```


#3.1
Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV,radio, and newspaper, rather than in terms of the coefficients of the linear model.

From Table 3.4, the null hypothesis for “TV” is that with the “radio” and “newspaper” existing, “TV” advertising has no influence on sales. The null hypothesis for “radio” is that with the “TV” and “newspaper” existing, “radio” advertising has no effect on sales. And the null hypothesis for “newspaper” is that with the “TV” and “radio” existing, “newspaper” advertising does not affect sales. The low p-values of “TV” and “radio” suggest that we should reject the null hypothesis for “TV” and “radio” and the high p-value of “newspaper” suggests that we should accept the null hypothesis for “newspaper”.

#3.2
Carefully explain the differences between the KNN classifier and KNN regression methods.

The main difference is the fact that for the classifier approach, the algorithm assumes the outcome as the class of more presence, and on the regression approach the response is the average value of the nearest neighbors.

#3.5
Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $i$th fitted value takes the form $\hat y_i=x_i \hat \beta$, where $\hat \beta= \frac {\sum{x_i y_i}}{\sum{x_i^2}}$.Show that we can write $\hat y_i=\sum{a_j y_j}$. What is a_j?
Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.

For the $i$th fitted value takes the form $\hat y_i=X_i \hat \beta$, where $\hat \beta= \frac{\sum{x_i y_i}}{\sum{x_j^2}}$. And$\hat y_i=\sum{a_j y_j}$. We have,$\hat y_i= X_i \hat \beta=X_i \frac{\sum{x_i y_i}}{\sum{x_j^2}}=\sum{\frac{x_j x_i}{\sum{x_k^2}}} y_j$
Because variables' names do not matter inside of $\sum$, we have,$a_j= \frac{x_i x_j}{\sum{x_k^2}}$

#3.6
Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar x, \bar y)$

We have equation1:$y=\beta_0+\beta_1 x$, and from (3.4), we got that equation2:$\hat \beta_0= \bar y-\hat \beta_1 \bar x$, also $\hat \beta_1= \frac {\sum{(x_i-\bar x)(y_i- \bar y)}}{\sum {(x_i- \bar x)^2}}$. So from equation1,we have, $0=\beta_0+\beta_1 x-y$. If $(\bar x,\bar y)$ is on the line, we have: $0=\beta_0+\beta_1 \bar x-\bar y$. Also from equation2, we have$\bar y=\beta_0+\beta_1 \bar x$. So, we get $0=\beta_0+\beta_1 \bar x-(\beta_0+\beta_1 \bar x)$, $0=0$, which makes sense.
In this case, we have a conclusion that the least squares line always passes through $(\bar x,\bar y)$.

#3.11
```{r echo=FALSE}
set.seed(1)
x = rnorm(100)
y = 2*x + rnorm(100)
```

###(a)
```{r echo=FALSE}
fit1 = lm(y~x + 0)
summary(fit1)
```
The p-value of t-statistic is almost zero so we should reject the null hypothesis.

###(b)
```{r echo=FALSE}
fit2 = lm(x ~ y + 0)
summary(fit2)
```
The p-value of t-statistic is near zero so we should reject the null hypothesis.

###(c)

Both outcomes from (a) and (b) are the same thing. $$y = 2*x + \epsilon $$ , which equals to,  $$x = 0.5 * (y - \epsilon) $$.

###(d)

For the regression of Y onto X without an intercept, the t-statistic for $H_0: \beta =0$ takes the form $t = \beta / SE(\beta)$, where $\hat \beta$ is given by (3.38), and where$SE(\hat \beta) = \sqrt{\frac {\sum{(y_i -x_i \hat \beta)^2}} {(n-1) \sum{x_i^2}}}$. Firstly, show algebraically, the t-statistic can be written as $\frac {\sqrt{n-1} \sum{x_i y_i}}{\sqrt{\sum{x_i^2} \sum{(y_i^2 - 2 \beta x_i y_i + x_i^2 \beta^2)}}}$. We have $t = {\frac {\sum{x_i y_i}} {\sum{x_i^2}}} {\sqrt{\frac {(n-1) \sum{x_i^2}} {\sum{(y_i -x_i \beta)^2}}}}$, and$t = \frac {\sqrt{n-1} \sum{x_i y_i}}{\sqrt{\sum{x_i^2} \sum{(y_i^2 - 2 \beta x_i y_i + x_i^2 \beta^2)}}}$, next we get$t= \frac {\sqrt{n-1} \sum{x_i y_i}}{\sqrt{\sum{x_i^2} \sum{y_i^2} -\sum{x_i y_i}  (2 \sum{x_i y_i} - \sum{x_i y_i})}}$.

```{r echo=FALSE}
(sqrt(length(x)-1) * sum(x*y)) / (sqrt(sum(x*x) * sum(y*y) - (sum(x*y))^2))
```

###(e)

Here we change y into x and x into y, the formula from (d) will not change any more.

###(f)
```{r echo=FALSE}
fit3 = lm(y~x)
summary(fit3)
fit4 = lm(x~y)
summary(fit4)
```
The results of t-statistic are the same.

#3.12

###(a)

When the sum of the squares of the observed y are equal to the sum of the squares of the observed x, the coefficients will be the same.

###(b)
```{r echo=FALSE}
set.seed(1)
x = rnorm(100)
y = 3*x
fit5 = lm(y~x + 0)
summary(fit5)
fit6 = lm(x~y + 0)
summary(fit6)
```

###(c)
```{r echo=FALSE}
set.seed(1)
x = rnorm(100)
y = sample(x, 100)
sum(x^2)
sum(y^2) # make the sums of both squares are identical
fit7 = lm(y ~ x + 0)
summary(fit7)
fit8 = lm(x ~ y + 0)
summary(fit8)
```
#3.13

###(a)
```{r echo=FALSE}
set.seed(1)
x = rnorm(100)
```

###(b)
```{r echo=FALSE}
eps = rnorm(100, 0, sqrt(0.25))
```

###(c)
```{r echo=FALSE}
y = -1 + 0.5*x + eps
```

The length of y is 100. $$\beta_0= -1 $$,and $$\beta_1 = 0.5 $$.

###(d)
```{r echo=FALSE}
plot(x,y)
```
There is a linear relationship between y and x with a positive slope.

###(e)
```{r echo=FALSE}
fit9 = lm(y~x)
summary(fit9)
```
This model has a coefficient that is close to that we generated in (c), and the p-values are near zero so we could reject the null hypothesis.

###(f)
```{r echo=FALSE}
plot(x,y)
abline(fit9, lwd = 2, col = 2)
abline(-1, 0.5, lwd = 2, col = 3)
legend(-1, legend = c("model fitting", "population"), col = 2:3, lwd = 2)
```
###(g)
```{r echo=FALSE}
fit10 = lm(y ~ x + I(x^2))
summary(fit10)
```

The model fitting is better after adding $x^2$ into the model because the $R^2$ increased slightly, but the $RSE$ decreased a little. The p-value of the t-statistic  is 0.164 which means there isn't any relationship between y and $x^2$.

###(h)
```{r echo=FALSE} 
set.seed(1)
eps1 = rnorm(100, 0, 0.1)
x1 = rnorm(100)
y1 = -1 + 0.5*x1 + eps1
plot(x1, y1)
fit11 = lm(y1~x1)
summary(fit11)
abline(fit11, lwd = 2, col = 2)
abline(-1, 0.5, lwd = 2, col = 3)
legend(-1, legend = c("model fitting", "population"), col = 2:3, lwd = 2)
```

From the result, we can find that the $RSE$ decreases considerably, changing into 0.09.

###(i)
```{r echo=FALSE}
set.seed(1)
eps2 = rnorm(100, 0, 0.5)
x2 = rnorm(100)
y2 = -1 + 0.5*x2 + eps2
plot(x2, y2)
fit12 = lm(y2~x2)
summary(fit12)
abline(fit12, lwd = 2, col = 2)
abline(-1, 0.5, lwd = 2, col = 3)
legend(-1, legend = c("model fitting", "population"), col = 2:3, lwd = 2)
```

The error observed in $R^2$ and $RSE$ increased considerably.

###(j)
```{r echo=FALSE}
confint(fit9)
confint(fit11)
confint(fit12)
```

The second and third fits' intervals are both narrower than the first fit's interval,although the third one is similar with the first one. All intervals seem to be centered on approximately 0.5.

#3.14

###(a)
```{r echo=FALSE}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

The regression coefficients are:
$\beta_0 = 2, \beta_1 =2, \beta_3 = 0.3$

###(b)
```{r echo=FALSE}
cor(x1, x2)
plot(x1, x2)
```

###(c)
```{r echo=FALSE}
fit13 = lm(y~ x1 + x2)
summary(fit13)
```

$\beta_0 = 2.1305, \beta_1 = 1.4396, \beta_2 = 1.0097$

The regression coefficients are similar to the true values, but with some standard error. For the p-values of $\beta_0$ and $\beta_1$, they are both below 5%, so we should reject the null hypothesis for these two coefficients. However, for $\beta_2$, the p-value is 0.3754, much above 5% typical cutoff, we cannot reject the null hypothesis for this coefficient.

###(d)
```{r echo=FALSE}
fit14 = lm(y ~ x1)
summary(fit14)
```
Yes, the null hypothesis for this regression coefficient can be rejected because the p-value for its t-statistic is close to zero.

###(e)
```{r echo=FALSE}
fit15 = lm(y ~ x2)
summary(fit15)
```

Yes, we can reject the null hypothesis for this regression given the p-value for its t-statistic is near zero.

###(f)

No, it doesn't contradict. Because there is a linear relationship between x1 and x2, it's difficult to distinguish their influences on the model when using them to make regression at the same time. But when they are regressed separately, it is more clearly to see the relationship between y and each predictor.

###(g)
```{r echo=FALSE}
x1 = c(x1, 0.1)
x2 = c(x2, 0.8)
y = c(y, 6)
fit16 = lm(y ~ x1 + x2)
summary(fit16)
fit17 = lm(y ~ x1)
summary(fit17)
fit18 = lm(y ~ x2)
summary(fit18)
```

For the first regressed model that including x1 and x2, the coefficient of x1 is not significant but that of x2 is statistically significant.

```{r echo=FALSE}
par(mfrow = c(2,2))
plot(fit16)
```
```{r echo=FALSE}
par(mfrow=c(2,2))
plot(fit17)
```
```{r echo=FALSE}
par(mfrow=c(2,2))
plot(fit18)
```
From the leverage plot of these three models, we can say the second one -- the point does not become a high leverage point. And the first and third ones do.
