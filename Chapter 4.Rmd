---
title: "Chapter 4"
author: "Tong Sun"
date: "1/27/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

required:4.6 4.8 4.9 4.13 4.14 4.15 4.16
optional:

##4.6 
Suppose we collect data for a group of students in a statistics class
with variables X1 = hours studied, X2 = undergrad GPA, and Y =
receive an A. We fit a logistic regression and produce estimated
coefficient, βˆ0 = −6, βˆ1 = 0.05, βˆ2 = 1.

###(a)
Estimate the probability that a student who studies for 40 h and
has an undergrad GPA of 3.5 gets an A in the class.

Here we have $X_1 = 40$ , $X_2 = 3.5$ and we have the equation for predicted probability is $Y = -6 + 0.05*X_1 + X_2$. Plugging the predictors' values in the equation we get:

$$
P(X) = \frac {e^{-6+0.05*X_1+X_2}}
             {1+e^{-6+0.05*X_1+X_2}}
    = 0.3775
        
$$
So the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class was 0.3775.

###(b)
How many hours would the student in part (a) need to study to
have a 50 % chance of getting an A in the class?

The equation for predicted probability tells us that:
$$
\frac{e^{-6+0.05*X_1+3.5}}
{1 + e^{-6+0.05*X_1+3.5}}
=0.5
$$
which means:
$$
e^{-6+0.05*X_1+3.5} = 1
$$
After taking the logarithm of both sides, we have:
$$
X_1 = \frac{2.5}{0.05} = 50
$$
We can make the conclusion that the student in part (a) need to study 50 hours in order to have a 50% chance of getting an A in the class.

##4.8
Suppose that we take a data set, divide it into equally-sized training
and test sets, and then try out two different classification procedures.First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. K=1
) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations ? Why ?

We have:
Logistic Regression: 20% training error rate, 30% test error rate
KNN(K=1): 18% average error rate

The nearest neighbor of any training observation should be the observation itself, so for KNN with K=1, the training error rate is zero.In this situation, the test error rate will be 36% in order to make the average error rate is 18%. As a result, I wil choose logistic regression so that the test error rate will be lower.

##4.9 
This problem has to do with odds.
###(a)
On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?

We have the equation:
$$
\frac{P(X)}{1-P(X)}= 0.37
\\
P(X)= \frac{0.37}{1+0.37} = 0.27
$$
We can make the conclusion: on average , a fraction of 27% of people defaulting on their credit card payment.

###(b)
Suppose that an individual has a 16 % chance of defaulting on her credit card payment. What are the odds that she will default?

We have $P(X)=0.16$, so:
$$
\frac{P(X)}{1-P(X)}= \frac{0.16}{1-0.16}=0.19
$$
The odds that she will default is 19% if she has a 16% chance of defaulting on her credit card payment.
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
